{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FineTuning_BERT_with_GPU.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9e034a2e1e1f436e9efc9e152cbd033c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_db8ac80c829e43d2854d82b2a5fca8fe",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_42fd96b88fa049919021ba0566091859",
              "IPY_MODEL_7d9f8597525147cfb41cafd2fc558d37"
            ]
          }
        },
        "db8ac80c829e43d2854d82b2a5fca8fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "42fd96b88fa049919021ba0566091859": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_32f15fb039b14a869c7cb35b3cae590f",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6d2b9bbd38b44f8eac037e1771a39750"
          }
        },
        "7d9f8597525147cfb41cafd2fc558d37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5b9342988031441a9d5650151e6a49b7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 232k/232k [00:00&lt;00:00, 2.61MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f73ef36db3a345ae8f3b0fd5f43e550d"
          }
        },
        "32f15fb039b14a869c7cb35b3cae590f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6d2b9bbd38b44f8eac037e1771a39750": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5b9342988031441a9d5650151e6a49b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f73ef36db3a345ae8f3b0fd5f43e550d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "14567ac1efb845f09d090f7523137318": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_34770ff6b28a493c945f92a54a824b06",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_77a93af79b11467c8cbb83d579b95e6b",
              "IPY_MODEL_8db729e215f8455282268697922205fe"
            ]
          }
        },
        "34770ff6b28a493c945f92a54a824b06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "77a93af79b11467c8cbb83d579b95e6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_210729f7d0bc471499fbc9e8553879fa",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_08c1a561185b4bb38ef3a06191325309"
          }
        },
        "8db729e215f8455282268697922205fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4700c561262c45cf9f1328dd05ce6699",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 361/361 [00:00&lt;00:00, 10.2kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_af3576f57f894dce9d232118ebb514d2"
          }
        },
        "210729f7d0bc471499fbc9e8553879fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "08c1a561185b4bb38ef3a06191325309": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4700c561262c45cf9f1328dd05ce6699": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "af3576f57f894dce9d232118ebb514d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "86f997959d9646a69736eda839dfbe89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0b0a087ca6274d6997fb3c3baeb25036",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9fe5ff47ff0242deb0a057883d36d9bd",
              "IPY_MODEL_73382929c78247ad93d390d08c033565"
            ]
          }
        },
        "0b0a087ca6274d6997fb3c3baeb25036": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9fe5ff47ff0242deb0a057883d36d9bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9c64093ded664d63b2e6f78d4c15cd83",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_37e6776ee9f9402cbdd35cee78c831f0"
          }
        },
        "73382929c78247ad93d390d08c033565": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a1f186c9d20442bfb7cdf3fe5dcaff63",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 440M/440M [00:06&lt;00:00, 70.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_671410693bef409ebdc085881aa934d6"
          }
        },
        "9c64093ded664d63b2e6f78d4c15cd83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "37e6776ee9f9402cbdd35cee78c831f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a1f186c9d20442bfb7cdf3fe5dcaff63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "671410693bef409ebdc085881aa934d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IKfENoTDap8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "73c43c47-0be0-4a06-b48f-f0d648204c6d"
      },
      "source": [
        "import tensorflow as tf \n",
        "\n",
        "#Get the GPU device name \n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81tHcJ2gD8EI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "26387174-d24a-4764-f139-f4eb7938a3be"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok8ECtxEEDnv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "outputId": "fcc6a075-5605-4fad-a05d-cfba87501639"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 50.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 49.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 57.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.2)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.9 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.9)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.9->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.9->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=d1a4fbbe52948b71080c8310c1cbf31f132e98ee429611f593ec638339d8c776\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.0.11 transformers-2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLiZh6_mEkk6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "49e59064-83ce-4510-8924-918a5d5fa01c"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=cebb32d2e4553d75bb4fb4b073889f3e8034fbb7da16d7a8aeebdbced239f7bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIIemZovOy6L",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "26b253fa-f19b-442b-f140-8b7fc3736f59"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-67969996-7bba-44e4-8892-7c24fbfbea8a\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-67969996-7bba-44e4-8892-7c24fbfbea8a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving in_domain_train.tsv to in_domain_train.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZBMN1crO0N-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8Au35VfRX_d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "b6ca4e80-e4a4-479e-a8cc-e0ba2c1f6672"
      },
      "source": [
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 8,551\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2006</th>\n",
              "      <td>rhl07</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Fred threw the ball under the porch.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8242</th>\n",
              "      <td>ad03</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Benjamin gave the cloak to Lee.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3882</th>\n",
              "      <td>ks08</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>They eager to leave the meeting.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4289</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The cat seems to be out of the bag.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>876</th>\n",
              "      <td>bc01</td>\n",
              "      <td>0</td>\n",
              "      <td>*?</td>\n",
              "      <td>Sally explained the attempt to arrest Holly, b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1501</th>\n",
              "      <td>r-67</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Beans I don't think you'll be able to convince...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>920</th>\n",
              "      <td>bc01</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Sally will stand near Mag, but he won't Holly.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6700</th>\n",
              "      <td>m_02</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The Romans built this aqueduct.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6032</th>\n",
              "      <td>c_13</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Did Calvin eat the beef waffles?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5553</th>\n",
              "      <td>b_73</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>They may grow as much as bamboo high.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "2006           rhl07  ...               Fred threw the ball under the porch.\n",
              "8242            ad03  ...                    Benjamin gave the cloak to Lee.\n",
              "3882            ks08  ...                   They eager to leave the meeting.\n",
              "4289            ks08  ...                The cat seems to be out of the bag.\n",
              "876             bc01  ...  Sally explained the attempt to arrest Holly, b...\n",
              "1501            r-67  ...  Beans I don't think you'll be able to convince...\n",
              "920             bc01  ...     Sally will stand near Mag, but he won't Holly.\n",
              "6700            m_02  ...                    The Romans built this aqueduct.\n",
              "6032            c_13  ...                   Did Calvin eat the beef waffles?\n",
              "5553            b_73  ...              They may grow as much as bamboo high.\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Qg5WsOkRamb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "c7cd34f7-94bd-4510-88f1-8d37f3eb0ca7"
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5490</th>\n",
              "      <td>Sally is such a fool as people think.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8196</th>\n",
              "      <td>Aphrodite stinks to be omnipotent.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1767</th>\n",
              "      <td>Willy is taller than I know a boy who is.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>645</th>\n",
              "      <td>Rub the baby with the cloth asleep.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8505</th>\n",
              "      <td>The constantly reading Shakespeare satisfied me</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             sentence  label\n",
              "5490            Sally is such a fool as people think.      0\n",
              "8196               Aphrodite stinks to be omnipotent.      0\n",
              "1767        Willy is taller than I know a boy who is.      0\n",
              "645               Rub the baby with the cloth asleep.      0\n",
              "8505  The constantly reading Shakespeare satisfied me      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfx6OZRVReHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TqSgK8ZRiga",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "9e034a2e1e1f436e9efc9e152cbd033c",
            "db8ac80c829e43d2854d82b2a5fca8fe",
            "42fd96b88fa049919021ba0566091859",
            "7d9f8597525147cfb41cafd2fc558d37",
            "32f15fb039b14a869c7cb35b3cae590f",
            "6d2b9bbd38b44f8eac037e1771a39750",
            "5b9342988031441a9d5650151e6a49b7",
            "f73ef36db3a345ae8f3b0fd5f43e550d"
          ]
        },
        "outputId": "0356da6e-e3d1-41fd-f78d-a44f36700902"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e034a2e1e1f436e9efc9e152cbd033c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QbzyvPnRy_I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "a5e3ced3-f35b-42df-de17-413ba2284685"
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Tokenized:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmT0kPy0R24A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 128  ##-->  Training epochs take ~5:28 each, score is 0.535\n",
        "MAX_LEN = 64   ##-->  Training epochs take ~2:57 each, score is 0.566"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x8H-HjnSChS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1e014130-93cc-41d1-82a5-5e35b96fcb38"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Token IDs: [101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuTCeBDKSdY0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f86d5ef3-8205-47c7-f9ef-85d4ffe95350"
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzP5h-X6S0GG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "39bf82f5-e4d3-4001-f6cf-057a40890a33"
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 64\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH5yH1hRS3xF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-7bZoIaS_o0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpRmouJBTEqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7Guge2zTNJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2ZPivCSTc3X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "14567ac1efb845f09d090f7523137318",
            "34770ff6b28a493c945f92a54a824b06",
            "77a93af79b11467c8cbb83d579b95e6b",
            "8db729e215f8455282268697922205fe",
            "210729f7d0bc471499fbc9e8553879fa",
            "08c1a561185b4bb38ef3a06191325309",
            "4700c561262c45cf9f1328dd05ce6699",
            "af3576f57f894dce9d232118ebb514d2",
            "86f997959d9646a69736eda839dfbe89",
            "0b0a087ca6274d6997fb3c3baeb25036",
            "9fe5ff47ff0242deb0a057883d36d9bd",
            "73382929c78247ad93d390d08c033565",
            "9c64093ded664d63b2e6f78d4c15cd83",
            "37e6776ee9f9402cbdd35cee78c831f0",
            "a1f186c9d20442bfb7cdf3fe5dcaff63",
            "671410693bef409ebdc085881aa934d6"
          ]
        },
        "outputId": "7e89baed-3693-4730-aec8-79ad39c59a8a"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14567ac1efb845f09d090f7523137318",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=361, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86f997959d9646a69736eda839dfbe89",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G34pdX2aTmtD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "outputId": "b6075d47-843a-4737-89af-606cfa2d2450"
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrV7iLvlTw7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic-foaEdT3AO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rXNYUMvT6Md",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X2mpkbRT9lf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avZRuv7kUBgR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9d7245fa-4bdd-4c0c-9167-da3092a344ab"
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:16.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:33.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:49.\n",
            "  Batch   160  of    241.    Elapsed: 0:01:05.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:21.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:37.\n",
            "\n",
            "  Average training loss: 0.49\n",
            "  Training epcoh took: 0:01:37\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:16.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:32.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:48.\n",
            "  Batch   160  of    241.    Elapsed: 0:01:04.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:20.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:37.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epcoh took: 0:01:37\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:16.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:32.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:48.\n",
            "  Batch   160  of    241.    Elapsed: 0:01:04.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:20.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:37.\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epcoh took: 0:01:37\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:16.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:32.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:48.\n",
            "  Batch   160  of    241.    Elapsed: 0:01:04.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:20.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:36.\n",
            "\n",
            "  Average training loss: 0.13\n",
            "  Training epcoh took: 0:01:37\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.83\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hhL-wdTVx6L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "53ed8748-03b8-4823-b62c-1e677a13bf2e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvAAAAGaCAYAAABpIXfbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngCciIiIiMiD/H1BbffOi/eilAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIIdaInFWitj",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "224030d9-3c83-419c-dfdd-17683d5c80d7"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6ca941a7-b6e3-4482-b3ed-bf53bf6b39b1\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-6ca941a7-b6e3-4482-b3ed-bf53bf6b39b1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving out_of_domain_dev.tsv to out_of_domain_dev.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sukWTuQHV5HN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c577f923-c671-4413-d959-c93bae89dba8"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 516\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgRJIc2lVz-T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ede8af19-9a89-49d8-8438-ac2c035c781d"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 516 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU-eR5oTW2LS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "305059ee-9c73-480e-9ec7-8e81de64da61"
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive samples: 354 of 516 (68.60%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27VNpMtQXE_x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "f9e002fa-8c75-4880-a7ec-908427c94a9f"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrVnBrdAXGD3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "9ff7a54e-27b7-4429-be46-6fa995436b9e"
      },
      "source": [
        "matthews_set"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.049286405809014416,\n",
              " -0.21684543705982773,\n",
              " 0.4732058754737091,\n",
              " 0.34151450937027694,\n",
              " 0.49517597397212765,\n",
              " 0.7410010097502685,\n",
              " 0.5269860393922079,\n",
              " 0.47519096331149147,\n",
              " 1.0,\n",
              " 0.7530836820370708,\n",
              " 0.7679476477883045,\n",
              " 0.647150228929434,\n",
              " 0.8749672939989046,\n",
              " 0.8320502943378436,\n",
              " 0.3268228676411533,\n",
              " 0.6625413488689132,\n",
              " 0.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guAzN0bFXMPm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6284a6a5-a122-4d94-836f-0cab520ff8f7"
      },
      "source": [
        " #Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR7vL7piXSv-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "6e75ecb3-7617-4f4a-9af1-370067282ae9"
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to ./model_save/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./model_save/vocab.txt',\n",
              " './model_save/special_tokens_map.json',\n",
              " './model_save/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOX8korEXXrQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "ae43cde4-7db3-4d62-ebe8-7ce997cf18aa"
      },
      "source": [
        "!ls -l --block-size=K ./model_save/"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 427960K\n",
            "-rw-r--r-- 1 root root      2K Feb  7 01:56 config.json\n",
            "-rw-r--r-- 1 root root 427719K Feb  7 01:56 pytorch_model.bin\n",
            "-rw-r--r-- 1 root root      1K Feb  7 01:56 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      1K Feb  7 01:56 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root    227K Feb  7 01:56 vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0gNwt06XeE8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "a65bacd2-ddc3-4fb4-9bb6-7c810794c0b6"
      },
      "source": [
        "# Mount Google Drive to this Notebook instance.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjH2MRIjXxyx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "13cbf5fb-7221-4b75-d8ce-ade9099e2b78"
      },
      "source": [
        "#!cp -r ./model_save/ \"./drive/Shared drives/\""
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot create directory './drive/Shared drives/': Operation not supported\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XckMOmFCYUPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load a trained model and vocabulary that you have fine-tuned\n",
        "model = model_class.from_pretrained(output_dir)\n",
        "tokenizer = tokenizer_class.from_pretrained(output_dir)\n",
        "\n",
        "# Copy the model to the GPU.\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}